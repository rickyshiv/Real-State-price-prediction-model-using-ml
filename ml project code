import pandas as pd

housing=pd.read_csv("C:\\Users\\RICKY\\Desktop\\data.csv")

#this willl gives first 5 rows
housing.head()

housing.info()
#this gives us the information and from this we can find missing data

#this will gives us the number of 0 and 1
housing['CHAS'].value_counts()

housing.describe()

%matplotlib inline

#for plotting histogram
#import matplotlib.pyplot as plt
#housing.hist(bins=50,figsize=(20,15))

## Train-Test Splitting

# for learning purpose we have to take data into two parts as test data and train data we have to train 
# only 80% data and 20% for test data.
import numpy as np
def split_train_test(data,test_ratio):
    np.random.seed(42)
    shuffled=np.random.permutation(len(data))
  
    test_set_size=int(len(data)*test_ratio)
    test_indices=shuffled[:test_set_size]
    train_indices=shuffled[test_set_size:]
    return data.iloc[train_indices],data.iloc[test_indices]

#train_set,test_set=split_train_test(housing,0.2)

#print(f"rows in train set:{len(train_set)}\nRows in test set:{len(test_set)}\n")

from sklearn.model_selection import train_test_split
train_set,test_set=train_test_split(housing,test_size=0.2,random_state=42)
print(f"rows in train set:{len(train_set)}\nRows in test set:{len(test_set)}\n")


from sklearn.model_selection import StratifiedShuffleSplit
split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
for train_index, test_index in split.split(housing, housing['CHAS']):
    strat_train_set = housing.loc[train_index]
    strat_test_set = housing.loc[test_index]
    

strat_test_set['CHAS'].value_counts()

strat_train_set['CHAS'].value_counts()

#95/7

#376/28


housing=strat_train_set.copy()

# looking for correlations

corr_matrix=housing.corr()

corr_matrix['MEDV'].sort_values(ascending=False)

from pandas.plotting import scatter_matrix
attributes = ["MEDV", "RM", "ZN", "LSTAT"]
scatter_matrix(housing[attributes], figsize = (12,8))
   

# as by plotting the scaater plot we get to know that some points are same so to remove this type of
# negatives data points we correlates.
housing.plot(kind="scatter",x="RM",y="MEDV",alpha=0.8)

## Trying out attribute combinations

housing["TAXRM"]=housing['TAX']/housing['RM']


housing.head()

corr_matrix=housing.corr()
corr_matrix['MEDV'].sort_values(ascending=False)

housing.plot(kind="scatter",x="TAXRM",y="MEDV",alpha=0.8)

housing=strat_train_set.drop("MEDV",axis=1)
housing_labels=strat_train_set["MEDV"].copy()

## Missing Attributes

# to take care off attributes ,you have 3 options :
   # 1. get rid of the missing data points
    # 2. get rid of the whole attribute
   # 3. set the value to some value (0,mean or median)

a=housing.dropna(subset=["RM"]) # option 1
a.shape

b=housing.drop("RM",axis=1) # option 2
b.shape
# note that there is no RM column and also note that original data fram remain unchanged

median=housing["RM"].median# compute median for option 3
housing["RM"].fillna(median)# option 3
housing.shape

from sklearn.impute import SimpleImputer
imputer=SimpleImputer(strategy="median")
imputer.fit(housing)


imputer.statistics_


x=imputer.transform(housing)

housing_tr=pd.DataFrame(x,columns=housing.columns)


housing_tr.describe()

## scikit-learn design

primarily , three types of objects

1.Estimators- it estimates some parameter based on a dataset .eg = imputer
it has a fit method and transform method.
Fit method- fits the data set and calculates internal parameters

2.Transformers - transformer method takes input and returns output based on the learnings from fit() . it also has a convineice function called fit_transform() which fits and then transforms.

3.Predictors- linear regression model is a example of predictor .fit() and predict are two common functions .it also gives us score() function which will evaluate the predictions.


## FEATURE SCALING

Primarily, two types of feature scaling methods:
1. min-max scaling(normalization)
(value-min)/(max-min)
sklearn provide a class called minmax scaller 
2. standardization
(value-mean)/std
sklearn provides class standard scaler for this 

## creating a pipeline 

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
my_pipeline=Pipeline([
    ('Ã­mputer',SimpleImputer(strategy="median")),
    # add as many as you wanna want in pipeline 
    ('std_scaler',StandardScaler()),
    
])

housing_num_tr=my_pipeline.fit_transform(housing)

housing_num_tr


housing_num_tr.shape

## selecting a desired model for real estates

from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
#model=LinearRegression()
#model=DecisionTreeRegressor()
from sklearn.ensemble import RandomForestRegressor
model=RandomForestRegressor()
model.fit(housing_num_tr,housing_labels)

some_data=housing.iloc[:5]

some_labels=housing_labels.iloc[:5]

prepared_data=my_pipeline.transform(some_data)

model.predict(prepared_data)

list(some_labels)


## Evaluating the model

from sklearn.metrics import mean_squared_error
housing_predictions=model.predict(housing_num_tr)
mse=mean_squared_error(housing_labels,housing_predictions)
rmse=np.sqrt(mse)

rmse

## using better evaluation technique = cross validation

# 1 2 3 4 5 6 7 8
from sklearn.model_selection import cross_val_score
scores=cross_val_score(model,housing_num_tr,housing_labels,scoring="neg_mean_squared_error",cv=10)
rmse_scores=np.sqrt(-scores)

rmse_scores

def print_scores(scores):
    print("scores",scores)
    print("mean:",scores.mean())
    print("standard deviation:",scores.std())

print_scores(rmse_scores)

## saving the model

from joblib import dump,load
dump(model,'Dragon.joblib')

## Testing the model on test data

x_test=strat_test_set.drop("MEDV",axis=1)
y_test=strat_test_set["MEDV"].copy()
x_test_prepared=my_pipeline.transform(x_test)
final_predictions=model.predict(x_test_prepared)
final_mse=mean_squared_error(y_test,final_predictions)
final_rmse=np.sqrt(final_mse)
# print(final_predictions,list(y_test))

final_rmse

prepared_data[0]

## using the model

from joblib import dump,load
import numpy as np
model=load('Dragon.joblib')
features=np.array([[-0.43942006,  7.12628155, -1.12165014, -0.137288841, -1.42262747,
       -11.53979304, -99.31238772,  2.61111401, -26.0016859 , -0.5778192 ,
       -0.97491834,  0.41164221, -0.86091034]])
model.predict(features)

